[{"authors":["admin"],"categories":null,"content":"I am a postdoctoral research fellow working with Assistant Professor Lee Gim Hee in the Computer Vision and Robot Perception (CVRP) Lab, Department of Computer Science, National University of Singapore (NUS). I received my PhD degree as an NUS NGS Scholar in the Adaptive Computing (AdaComp) Lab under the supervision of Professor David Hsu. Prior to PhD, I recevied my B.Comp. (Computer Science) and B.Sci. (Applied Mathematics) (1st Class Honors) from NUS. I also worked closely with Associate Professor Shengdong Zhao.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ziquan111.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a postdoctoral research fellow working with Assistant Professor Lee Gim Hee in the Computer Vision and Robot Perception (CVRP) Lab, Department of Computer Science, National University of Singapore (NUS). I received my PhD degree as an NUS NGS Scholar in the Adaptive Computing (AdaComp) Lab under the supervision of Professor David Hsu. Prior to PhD, I recevied my B.Comp. (Computer Science) and B.Sci. (Applied Mathematics) (1st Class Honors) from NUS.","tags":null,"title":"Ziquan LAN","type":"authors"},{"authors":[],"categories":[],"content":"","date":1562657243,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562657243,"objectID":"5524c78b8e633b256a97d3ea728cb720","permalink":"https://ziquan111.github.io/project/human-robot-interaction-in-domestic-environment/","publishdate":"2019-07-09T15:27:23+08:00","relpermalink":"/project/human-robot-interaction-in-domestic-environment/","section":"project","summary":"","tags":[],"title":"Human Robot Interaction in Domestic Environment","type":"project"},{"authors":[],"categories":[],"content":"","date":1562657220,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562657220,"objectID":"a1e8f9f4a42f64a4d76851c2d1bd9daa","permalink":"https://ziquan111.github.io/project/a-semantic-touch-interface-for-flying-camera-photography/","publishdate":"2019-07-09T15:27:00+08:00","relpermalink":"/project/a-semantic-touch-interface-for-flying-camera-photography/","section":"project","summary":"","tags":[],"title":"A Semantic Touch Interface for Flying Camera Photography","type":"project"},{"authors":[],"categories":[],"content":"","date":1562657201,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562657201,"objectID":"377f5f4e6b23a024ad253f94e5f30deb","permalink":"https://ziquan111.github.io/project/multi-robot-localization-and-perception/","publishdate":"2019-07-09T15:26:41+08:00","relpermalink":"/project/multi-robot-localization-and-perception/","section":"project","summary":"","tags":[],"title":"Multi-Robot Localization and Perception","type":"project"},{"authors":["Ziquan Lan","Zi Jian Yew","Gim Hee Lee"],"categories":[],"content":"","date":1560643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560643200,"objectID":"a2fb121462e313dc62407eff38bfd3b6","permalink":"https://ziquan111.github.io/publication/robust-point-cloud-based-reconstruction-of-large-scale-outdoor-scenes/","publishdate":"2019-06-16T00:00:00Z","relpermalink":"/publication/robust-point-cloud-based-reconstruction-of-large-scale-outdoor-scenes/","section":"publication","summary":"Outlier feature matches and loop-closures that survived front-end data association can lead to catastrophic failures in the back-end optimization of large-scale point cloud based 3D reconstruction. To alleviate this problem, we propose a probabilistic approach for robust back-end optimization in the presence of outliers. More specifically, we model the problem as a Bayesian network and solve it using the Expectation-Maximization algorithm. Our approach leverages on a long-tail Cauchy distribution to suppress outlier feature matches in the odometry constraints, and a Cauchy-Uniform mixture model with a set of binary latent variables to simultaneously suppress outlier loop-closure constraints and outlier feature matches in the inlier loop-closure constraints. Furthermore, we show that by using a Gaussian-Uniform mixture model, our approach degenerates to the formulation of a state-of-the-art approach for robust indoor reconstruction. Experimental results demonstrate that our approach has comparable performance with the state-of-the-art on a benchmark indoor dataset, and outperforms it on a large-scale outdoor dataset. Our source code can be found on the project website.","tags":[],"title":"Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes","type":"publication"},{"authors":["Ziquan Lan"],"categories":[],"content":"","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546214400,"objectID":"9eee9f3f4ceee056108ca9ceebfa923a","permalink":"https://ziquan111.github.io/publication/a-semantic-touch-interface-for-flying-camera-photography/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/publication/a-semantic-touch-interface-for-flying-camera-photography/","section":"publication","summary":"Compared with handheld cameras widely used today, a camera mounted on a flying drone affords the user much greater freedom in finding the point of view (POV) for a perfect photo shot. In the future, many people may take along compact flying cameras, and use their touchscreen mobile devices as viewfinders to take photos. To make this dream come true, the interface for photo-taking using flying cameras has to provide a satisfactory user experience.  In this thesis, we aim to develop a touch-based interactive system for photo-taking using flying cameras, which investigates both the user interaction design and system implementation issues. For interaction design, we propose a novel two-stage explore- and-compose paradigm. In the first stage, the user explores the photo space to take exploratory photos through autonomous drone flying. In the second stage, the user restores a selected POV with the help of a gallery preview and uses intuitive touch gestures to refine the POV and compose a final photo. For system implementation, we study two technical problems and integrate them into the system development: (1) the underlying POV search problem for photo composition using intuitive touch gestures; and (2) the obstacle perception problem for collision avoidance using a monocular camera.   The proposed system has been successfully deployed in indoor, semi-outdoor and limited outdoor environments for photo-taking. We show that our interface enables fast, easy and safe photo-taking experience using a flying camera.","tags":[],"title":"A Semantic Touch Interface for Flying Camera Photography","type":"publication"},{"authors":["Ziquan Lan","David Hsu","Gim Hee Lee"],"categories":[],"content":"","date":1529366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529366400,"objectID":"8ea758528dbd79701652564ce59e8289","permalink":"https://ziquan111.github.io/publication/solving-the-perspective-2-point-problem-for-flying-camera-photo-composition/","publishdate":"2018-06-19T00:00:00Z","relpermalink":"/publication/solving-the-perspective-2-point-problem-for-flying-camera-photo-composition/","section":"publication","summary":"Drone-mounted flying cameras will revolutionize photo-taking. The user, instead of holding a camera in hand and manually searching for a viewpoint, will interact directly with image contents in the viewfinder through simple gestures, and the flying camera will achieve the desired viewpoint through the autonomous flying capability of the drone. This work studies a common situation in photo-taking, i.e., the underlying viewpoint search problem for composing a photo with two objects of interest. We model it as a Perspective-2-Point (P2P) problem, which is under-constrained to determine the six degrees-of-freedom camera pose uniquely. By incorporating the user’s composition requirements and minimizing the camera’s flying distance, we form a constrained nonlinear optimization problem and solve it in closed form. Experiments on synthetic data sets and on a flying camera system indicate promising results.","tags":[],"title":"Solving the Perspective-2-Point Problem for Flying-Camera Photo Composition","type":"publication"},{"authors":["Ziquan Lan","Mohit Shridhar","David Hsu","Shengdong Zhao"],"categories":[],"content":"","date":1499817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499817600,"objectID":"f151c549935d4d09f37538ec1d2bd531","permalink":"https://ziquan111.github.io/publication/xpose-reinventing-user-interaction-with-flying-cameras/","publishdate":"2017-07-12T00:00:00Z","relpermalink":"/publication/xpose-reinventing-user-interaction-with-flying-cameras/","section":"publication","summary":"XPose is a new touch-based interactive system for photo taking, designed to take advantage of the autonomous flying capability of a drone-mounted camera. It enables the user to interact with photos directly and focus on taking photos instead of piloting the drone. XPose introduces a two-stage eXplore-and-comPose approach to photo taking in static scenes. In the first stage, the user explores the photo space through predefined interaction modes: Orbit, Pano, and Zigzag. Under each mode, the camera visits many points of view (POVs) and takes exploratory photos through autonomous drone flying. In the second stage, the user restores a selected POV with the help of a gallery preview and uses direct manipulation gestures to refine the POV and compose a final photo. Our prototype implementation, based on a Parrot Bebop quadcopter, relies mainly on a single monocular camera and works reliably in a GPS-denied environment. A systematic user study indicates that XPose results in more successful user performances in photo-taking tasks than the touchscreen joystick interface widely used in commercial drones today.","tags":[],"title":"XPose: Reinventing User Interaction with Flying Cameras","type":"publication"},{"authors":["Xiaoning Ma","Xin Yang","Shengdong Zhao","Chi-Wing Fu","Ziquan Lan","Yiming Pu"],"categories":[],"content":"","date":1346112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1346112000,"objectID":"9d124f11441b1555d15e10e9d5ef9947","permalink":"https://ziquan111.github.io/publication/robots-in-my-contact-list-using-social-media-platforms-for-human-robot-interaction-in-domestic-environment/","publishdate":"2012-08-28T00:00:00Z","relpermalink":"/publication/robots-in-my-contact-list-using-social-media-platforms-for-human-robot-interaction-in-domestic-environment/","section":"publication","summary":"This paper proposes to put domestic robots as buddies on our contact lists, thereby extending the use of social media in interpersonal interaction further to human-robot interaction (HRI). In detail, we present a robot management system that employs complementary social media platforms for human to interact with the vacuuming robot Roomba, and a surveillance robot which is developed in this paper on top of an iRobot Create. The social media platforms adopted include short message services (SMS), instant messenger (MSN), online shared calendar (Google Calendar), and social networking site (Facebook). Hence, our system can provide a rich set of user-familiar, intuitive and highly-accessible interfaces, allowing users to flexibly choose their preferred tools in different situations. An in-lab experiment and a multi-day field study are also conducted to study the characteristics and strengths of each interface, and to investigate the users’ perception to the robots and behaviors in choosing the interfaces during the course of HRI.","tags":[],"title":"Robots in My Contact List: Using Social Media Platforms for Human-Robot Interaction in Domestic Environment","type":"publication"}]